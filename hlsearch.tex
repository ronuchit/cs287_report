\section{Learning to Search the PRGraph}
\subsection{Formulation as MDP}
We adopt the same two-tiered decision strategy described in Section IV-C.
We formulate PRGraph search as an MDP as follows:
\begin{tightlist}
\item A state $s \in \St$ is a tuple $(PRG, r_{cur}, E)$, consisting of the
PRGraph with all its high-level plans, the current setting of
values for symbolic references for all high-level plans, and
an encoding of the geometric environment.
\item An action $a \in \A$ is a pair $(n, m)$, where $n$ is the selected node and $m$ is
the mode to apply (either trying to refine the node or quickly generating failure information).
\item If we select a node $n$ to try to refine, the transition function $T(s, a, s')$ is the same
as that in the plan refinement MDP presented in Chitnis et al.~\cite{chitnis2015mlpc} Briefly,
it describes how continuous values for symbolic references in node $n$'s plan are resampled, altering $r_{cur}$. If we instead
try to quickly generate failure information for $n$, then $T$ is defined by how we determine a geometric
fact to replan with at the task planner level. In our system, we sample a trajectory (which may have
collisions) for achieving each step in the plan, then roll out that trajectory in simulation and propagate
back to the task planner the first error we encounter (obstructing object, unreachable object, etc.).
\item The reward function $R(s, a, s')$ is 1 if $r_{cur}$ for any high-level plan in the PRGraph
has collision-free linking trajectories, and 0 otherwise.
\end{tightlist}

\subsection{Max-Margin Approach}
We exploit properties of the environment to hand-design a feature vector $f(n)$ that geometrically
describes aspects of a single high-level plan, thus encoding its refinability. Because the mode $m$ is binary,
we construct $$f((n, m)) = \begin{bmatrix} f(n) \\ f(n) \end{bmatrix}^\top \begin{bmatrix} 1 - m \\ m \end{bmatrix},$$
which stacks the feature vector for $n$ on itself, then turns off the bottom half when $m = 0$ and the
top half when $m = 1$. Now that we have defined a feature vector associated with each action that can be taken
from a state, we can obtain human-demonstrated trajectories (sequences of actions $(n, m)^{*}$) that intelligently
navigate the PRGraph. We then solve the following max-margin optimization problem with a structured margin and slack variables:
$$\min ||w||^{2} + C\sum_{i} \xi_{i}$$
$$\text{s.t.}\ w^{\top}f((n, m)_{i}^{*}) \geq w^{\top}f((n, m)_{ij}) + $$
$$d(f((n, m)_{i}^{*}), f((n, m)_{ij})) - \xi_{i}\ \forall i, j,\ \xi_{i} \geq 0\ \forall i$$
where the $i$ iterate over the demonstrated trajectories and the $j$ iterate over possible actions. Each
demonstrated trajectory has an associated slack variable in this formulation.
We note that the weight vector $w$ encodes a score function on the different actions, which produces
an ordering that matches the Q-function in the described MDP. Of course, the score function is not identical
to the Q-function, because we are not incorporating $R(s, a, s')$ in our formulation for $w$.

We use DAgger to augment our training data. In the first iteration, we navigate the PRGraph following
the expert actions and store these actions in a dataset. After some iterations, we use the dataset to solve for an
initial weight vector $w$. On subsequent iterations, we navigate the PRGraph by rolling out the policy
encoded by $w$ (pick the action with the highest score $w^{\top}f((n, m))$) while asking the expert what
the optimal action would be at each step, appending these into the dataset. After some iterations, we use the
aggregated dataset to solve for a new $w$, and repeat.

At test time, we follow the policy encoded by $w$ straightforwardly, picking the highest-scoring action
at each step.

\subsection{Logistic Regression Approach}
Within the framework of classification, a policy can be viewed as a classifier that discriminates between “good” and “bad” actions. While it is natural to model the quantity $p(action|state)$, the underlying state space doesn't have a fixed dimensionality across different instantiations of the problem and is difficult to express with a feature vector of constant length. Instead, we relax the problem and model the quantity $p(optimal|action)$. This is simply a binary classification problem where the label takes on two possible values: ``optimal'' and ``not optimal''. Using a logistic regression model we express the conditional probability as $$p(optimal=true|action;w) = \frac{1}{1+e^{-w^\top x}}$$ where $x = f((n,m))$ is the feature vector associated with the current action. To estimate the parameter vector $w$ we minimize the following cost function: $$\ell(w|D)=\lambda||w||^2_2-\sum_{ij} y_{ij}\log\mu_{ij}+(1-y_{ij})\log(1-\mu_{ij})$$ where $\mu_{ij}=1/(1+e^{-w^\top f((n,m)_{ij})})$ and $y_{ij}=1$ if the human took action $j$ in the $i$'th demonstration and $0$ otherwise. We make the assumption that humans performs optimally.

For this approach we do not explicitly use DAgger to augment the training data, instead we train on the largest data set that was collected. At test time, we evaluate a stochastic policy and a deterministic policy. The deterministic policy takes the action with the highest probability of being optimal. For the stochastic policy we normalize the probability of being optimal across all possible actions and randomly select an action from the resulting distribution.